{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9990ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from img2table.document import Image\n",
    "import io\n",
    "from PIL import Image as PIL_Image, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44d982d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_image(image, model):\n",
    "    \"\"\"\n",
    "    Classify the image to numbers from 0 to 8, 8 is None\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    image: ndarray of a single channel image.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    pred: classified value corresponding to input image\n",
    "    \n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    resized = cv2.resize(gray, (40, 40))  # 40x40 pixels is the input shape of model\n",
    "\n",
    "    # Expand the dimensions of the image to match the input shape of the model\n",
    "    im = np.expand_dims(resized, axis=0)\n",
    "\n",
    "    result = model.predict(im)\n",
    "    pred = np.argmax(result[0])\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe469d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cell_extraction_classification_df_return(orddict, img, model):\n",
    "    \"\"\"\n",
    "    Extracts cells of the image from the bbox values in orddict,\n",
    "    classify the image using our custom ocr model, and returns the result as a dataframe\n",
    "    \n",
    "    Parameter\n",
    "    ---------\n",
    "    orddict: ordered Dictionary having 4 values of bbox\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    df: dataframe of classified values\n",
    "    \n",
    "    \"\"\"\n",
    "    del orddict[0], orddict[4]  # del the first and last keys (rows) of orddict\n",
    "    pred = []\n",
    "    for key, cell_list in orddict.items():  # do the bbox extrn and classification using our model\n",
    "        for cell in cell_list:\n",
    "            x1 = cell.bbox.x1\n",
    "            y1 = cell.bbox.y1\n",
    "            x2 = cell.bbox.x2\n",
    "            y2 = cell.bbox.y2\n",
    "\n",
    "            new_im = img.crop((x1, y1, x2, y2))\n",
    "            im_arr = np.array(new_im)  # Converting new_im (PIL.Image.Image) to numpy array for predict_image()\n",
    "            pred.append(classify_image(im_arr, model))\n",
    "    pred = [0 if num == 8 else num for num in pred]\n",
    "    pred_arr = np.array(pred)\n",
    "    reshaped_pred_arr = pred_arr.reshape(3, 13)  # 3 rows and 13 columns\n",
    "    df = pd.DataFrame(reshaped_pred_arr)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e676f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataframe_postprocessing(paper_df):\n",
    "    \"\"\"\n",
    "    Preprocessing the dataframe - removing first column,\n",
    "    flattening the np.array of df column-wise,\n",
    "    and returns the values to be added to main mark-dictionary\n",
    "        \n",
    "    Parameter\n",
    "    ---------\n",
    "    paper_df: Output of cell_extraction_classification_df_return(), df with the unwanted first column\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    cell_vals: Values to be added to main mark-dictionary\n",
    "    \n",
    "    \"\"\"\n",
    "    paper_df = paper_df.iloc[:, 1:]  # iloc[row, column], removing first column\n",
    "\n",
    "    # Flattening & adding marks to my_dict\n",
    "    paper_arr = paper_df.to_numpy()\n",
    "    flat = paper_arr.flatten(order='F')  # F - flattening column-wise\n",
    "    cell_vals = [i for i in flat]\n",
    "\n",
    "    return cell_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ffe3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dictonary(imgs, model):\n",
    "    # Dictionary for storing marks of each papers\n",
    "    my_dict = {'1a': [], '1b': [], '1c': [], '2a': [], '2b': [], '2c': [], '3a': [], '3b': [], '3c': [], '4a': [],\n",
    "               '4b': [], '4c': [], '5a': [], '5b': [], '5c': [], '6a': [], '6b': [], '6c': [], '7a': [], '7b': [],\n",
    "               '7c': [], '8a': [], '8b': [], '8c': [], '9a': [], '9b': [], '9c': [], '10a': [], '10b': [], '10c': [],\n",
    "               '11a': [], '11b': [], '11c': [], '12a': [], '12b': [], '12c': []}\n",
    "\n",
    "    for i in range(len(imgs)):\n",
    "        img = imgs[i]\n",
    "        dpi = (200, 200)\n",
    "        img.info[\"dpi\"] = dpi\n",
    "        img_bytes = io.BytesIO()\n",
    "        img.save(img_bytes, format='JPEG')\n",
    "        img_bytes.seek(0)\n",
    "        doc = Image(img_bytes)\n",
    "        print(doc)\n",
    "        extracted_tables = doc.extract_tables(implicit_rows=False, min_confidence=50)\n",
    "        print(extracted_tables)\n",
    "        orddict = extracted_tables[0].content\n",
    "        img = PIL_Image.open(img_bytes)\n",
    "        if len(orddict.keys()) == 5 and sum(len(value) for value in orddict.values()) == 65:\n",
    "            # if the table recognition is correct, the no of rows will be 5\n",
    "            paper_df = cell_extraction_classification_df_return(orddict, img, model)\n",
    "            marks_for_main_dict = dataframe_postprocessing(paper_df)\n",
    "\n",
    "            for key, value in zip(my_dict.keys(), marks_for_main_dict):\n",
    "                my_dict[key].append(value)  # Adding values to dictionary\n",
    "\n",
    "        else:  # if the table recognition is incorrect\n",
    "            for key in my_dict.keys():\n",
    "                my_dict[key].append(0)  # Adding values to dictionary\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c83c4201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67d944d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"MP_Latest_Model.h5\")\n",
    "image_list = []\n",
    "dict_ret_df_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3772929d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list.append(PIL_Image.open(r\"sample_image.jpeg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7240c9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image(src=<_io.BytesIO object at 0x000001E60358B290>, detect_rotation=False)\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_dictonary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 19\u001b[0m, in \u001b[0;36mreturn_dictonary\u001b[1;34m(imgs, model)\u001b[0m\n\u001b[0;32m     17\u001b[0m extracted_tables \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mextract_tables(implicit_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, min_confidence\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(extracted_tables)\n\u001b[1;32m---> 19\u001b[0m orddict \u001b[38;5;241m=\u001b[39m \u001b[43mextracted_tables\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m     20\u001b[0m img \u001b[38;5;241m=\u001b[39m PIL_Image\u001b[38;5;241m.\u001b[39mopen(img_bytes)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(orddict\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m orddict\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m65\u001b[39m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# if the table recognition is correct, the no of rows will be 5\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "return_dict = return_dictonary(image_list, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6894815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
